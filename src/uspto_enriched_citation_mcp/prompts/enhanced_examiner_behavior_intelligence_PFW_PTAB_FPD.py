"""Enhanced Examiner Behavior Intelligence Prompt

Comprehensive examiner profiling combining prosecution patterns (PFW), citation behavior (Citations),
petition history (FPD), and PTAB challenge correlation for strategic prosecution planning.

This enhanced version includes:
- Robust error handling for missing citation data (pre-2017 OA coverage)
- Statistical significance validation (minimum thresholds)
- Progressive disclosure with user confirmation at key decision points
- Enhanced citation category analysis with strategic interpretations
- FPD integration for quality assessment
- PTAB integration for post-grant risk profiling
- Actionable recommendations based on data patterns
- Complete download workflows and presentation formatting
"""

from . import mcp


@mcp.prompt(
    name="enhanced_examiner_behavior_intelligence_PFW_PTAB_FPD",
    description="ENHANCED: Comprehensive examiner profiling with citation patterns, petition history, PTAB correlation, and strategic prosecution recommendations. At least ONE parameter required (examiner_name, art_unit, or technology_keywords). Citations data Oct 1, 2017+ only. Requires PFW, Citations, FPD, and PTAB MCPs.",
)
async def enhanced_examiner_behavior_intelligence_PFW_PTAB_FPD_prompt(
    examiner_name: str = "", art_unit: str = "", technology_keywords: str = ""
) -> str:
    """Generate comprehensive examiner profiles combining prosecution patterns, citation behavior,
    petition history, and PTAB challenge correlation for strategic prosecution planning.

    MANDATORY FIRST STEP: Validate the provided parameters and immediately begin analysis using
    any non-empty parameter. If ALL parameters are empty, ask the user to provide at least one
    search parameter.

    This is a COMPREHENSIVE MULTI-PHASE workflow:
    1. PFW: Get examiner's applications with wildcard search + ultra-minimal fields
    2. Citations: Analyze citation patterns with 2017+ date filtering
    3. PFW: NOA deep dive for allowance reasoning patterns
    4. PFW: Prosecution efficiency metrics (RCE, finals, amendments)
    5. FPD: Petition history for quality assessment
    6. PTAB: Post-grant challenge correlation
    7. Generate comprehensive intelligence report with strategic recommendations

    Args:
        examiner_name: Examiner last name or full name (e.g., 'SMITH' or 'SMITH, JOHN')
        art_unit: Optional art unit number for filtering (e.g., '2854')
        technology_keywords: Optional technology focus areas
    """

    # Validate inputs
    if not examiner_name and not art_unit and not technology_keywords:
        return """
# ENHANCED EXAMINER BEHAVIOR INTELLIGENCE SYSTEM

‚ùå **ERROR: Missing Search Parameters**

Please provide at least one search parameter:
- **Examiner Name**: Last name or full name (e.g., 'SMITH' or 'SMITH, JOHN')
- **Art Unit**: Art unit number (e.g., '2854')
- **Technology Keywords**: Technology focus areas

**Example Usage:**
```
examiner_name='MEKHLIN, ELI S'
art_unit='1759'
technology_keywords='semiconductor'
```

**Recommended:** Provide examiner name AND art unit for best results and fastest execution.
"""

    return f"""
# ENHANCED EXAMINER BEHAVIOR INTELLIGENCE SYSTEM

**Target Analysis:**
- **Examiner**: {examiner_name or "Not specified"}
- **Art Unit**: {art_unit or "Not specified"}
- **Technology Focus**: {technology_keywords or "Not specified"}

**Data Coverage Notes:**
- **Citations MCP:** Office actions from Oct 1, 2017+ only
- **FPD MCP:** Petition decisions (check FPD_get_guidance for date ranges)
- **PFW MCP:** Full prosecution history available
- **PTAB MCP:** Post-grant proceedings (IPR/PGR from 2012+)

---

## PHASE 1: EXAMINER DISCOVERY & PORTFOLIO ANALYSIS

### 1.1 Wildcard Examiner Search with Validation

**‚ö†Ô∏è CRITICAL:** Use wildcard search + ultra-minimal fields (99% token reduction)

```python
from collections import Counter
import statistics

# Extract last name for wildcard search
examiner_input = "{examiner_name}"
if ',' in examiner_input:
    last_name = examiner_input.split(',')[0].strip()
    full_name_search = examiner_input.replace(',', '').strip()
else:
    last_name = examiner_input.strip()
    full_name_search = last_name

# Build targeted query with art unit filter if provided
art_unit_input = "{art_unit}"
tech_keywords_input = "{technology_keywords}"

if art_unit_input:
    art_unit_prefix = art_unit_input[:3]  # "1759" ‚Üí "175" for broader coverage
    query = f'examinerNameText:{{{{last_name}}}}* AND groupArtUnitNumber:{{{{art_unit_prefix}}}}*'
else:
    query = f'examinerNameText:{{{{last_name}}}}*'

# Add technology filter if specified
if tech_keywords_input:
    query += f' AND inventionTitle:({{{{tech_keywords_input}}}})'

# Add filing date filter (2015+ accounts for 2-year lag to 2017+ citation data)
query += ' AND filingDate:[2015-01-01 TO *]'

print(f"üîç **Searching for examiner:** {{last_name}}")
print(f"üìã **Query:** `{{query}}`")
print()

# STEP 1: Get examiner's application portfolio (ULTRA-MINIMAL MODE)
try:
    results = pfw_search_applications_minimal(
        query=query,
        fields=['applicationNumberText', 'applicationMetaData.examinerNameText',
                'applicationMetaData.groupArtUnitNumber', 'patentGrantDate',
                'applicationMetaData.filingDate', 'patentNumber',
                'applicationMetaData.inventionTitle', 'applicationMetaData.appStatusDescText'],
        limit=100  # Increased for better sampling
    )

    total_found = results.get('searchResultsTotalSize', 0)
    applications = results.get('applications', [])

    print(f"‚úÖ **Found {{total_found}} total applications**")
    print(f"üì¶ **Retrieved {{len(applications)}} for analysis**")
    print()

except Exception as e:
    print(f"‚ùå **ERROR:** Failed to retrieve examiner applications")
    print(f"   Error details: {{str(e)[:200]}}")
    print()
    print("**Troubleshooting:**")
    print("  - Verify examiner name spelling")
    print("  - Check if examiner is in this art unit")
    print("  - Try broader search (examiner name only, no art unit filter)")
    print()
    # STOP HERE - cannot proceed without applications
    raise SystemExit("Cannot proceed without application data")

# Validate sufficient data
min_sample_size = 20
if len(applications) < min_sample_size:
    print(f"‚ö†Ô∏è **WARNING:** Only {{len(applications)}} applications found (minimum {{min_sample_size}} recommended)")
    print()
    print("**Options:**")
    print("  1. Continue with limited data (results may have low statistical confidence)")
    print("  2. Broaden search criteria (remove art unit filter, use broader tech keywords)")
    print("  3. Cancel and refine search parameters")
    print()
    print("**Recommendation:** Continue if >10 apps, otherwise broaden search")
    print()
    # USER DECISION POINT - Let user decide whether to continue
```

### 1.2 Portfolio Overview & Examiner Name Disambiguation

```python
# Extract examiner names from results
examiner_names = Counter([
    app.get('applicationMetaData', {{}}).get('examinerNameText')
    for app in applications
    if app.get('applicationMetaData', {{}}).get('examinerNameText')
])

# Check for multiple examiners with similar names
if len(examiner_names) > 1:
    print("‚ö†Ô∏è **Multiple examiners found with similar names:**")
    print()
    for name, count in examiner_names.most_common():
        pct = (count / len(applications)) * 100
        print(f"  - **{{name}}**: {{count}} apps ({{pct:.1f}}%)")
    print()

    primary_examiner = examiner_names.most_common(1)[0][0]
    print(f"üéØ **Recommendation:** Filter to primary examiner: '{{primary_examiner}}'")
    print()
    print("**To filter to specific examiner:**")
    print(f"```python")
    print(f"target_examiner = '{{primary_examiner}}'")
    print(f"applications = [app for app in applications")
    print(f"               if app.get('applicationMetaData', {{}}).get('examinerNameText') == target_examiner]")
    print(f"print(f'‚úÖ Filtered to {{{{len(applications)}}}} applications for {{{{target_examiner}}}}')")
    print(f"```")
    print()
    # USER DECISION POINT - Let user filter if desired
else:
    primary_examiner = examiner_names.most_common(1)[0][0] if examiner_names else "Unknown"
    print(f"‚úÖ **Single examiner identified:** {{primary_examiner}}")
    print()

# Analyze art unit distribution
art_units = Counter([
    app.get('applicationMetaData', {{}}).get('groupArtUnitNumber')
    for app in applications
    if app.get('applicationMetaData', {{}}).get('groupArtUnitNumber')
])

primary_art_unit = art_units.most_common(1)[0] if art_units else ("Unknown", 0)

# Status distribution
status_dist = Counter([
    app.get('applicationMetaData', {{}}).get('appStatusDescText')
    for app in applications
    if app.get('applicationMetaData', {{}}).get('appStatusDescText')
])

# Display portfolio overview
print("### Examiner Portfolio Overview")
print()
print(f"**Examiner:** {{primary_examiner}}")
print(f"**Primary Art Unit:** {{primary_art_unit[0]}} ({{primary_art_unit[1]}} apps, {{(primary_art_unit[1]/len(applications)*100):.1f}}%)")
print(f"**Art Units Covered:** {{len(art_units)}}")
print(f"**Total Applications Analyzed:** {{len(applications)}}")
print()

print("**Art Unit Distribution:**")
for unit, count in art_units.most_common(5):
    pct = (count / len(applications)) * 100
    print(f"  - Art Unit {{unit}}: {{count}} apps ({{pct:.1f}}%)")
print()

print("**Application Status Distribution:**")
for status, count in status_dist.most_common(5):
    pct = (count / len(applications)) * 100
    print(f"  - {{status}}: {{count}} ({{pct:.1f}}%)")
print()
```

---

## PHASE 2: CITATION PATTERN ANALYSIS (ENHANCED)

### 2.1 Citation Data Collection with Coverage Validation

**‚ö†Ô∏è DATE CONSTRAINT:** Citation API has office action dates from 2017-10-01 forward only!

```python
print("---")
print()
print("### Citation Behavior Analysis")
print()
print(f"üìÖ **Citation Data Coverage:** Oct 1, 2017+ office actions only")
print()

# Filter applications likely to have citation data
# Office actions typically occur 1-2 years after filing
citation_eligible_apps = [
    app for app in applications
    if app.get('applicationMetaData', {{}}).get('filingDate', '') >= '2015-01-01'
]

print(f"üéØ **Citation-Eligible Applications:** {{len(citation_eligible_apps)}} (filed 2015+)")
print()

# Sort by application number (descending) - higher numbers = more recent = better citation coverage
# Application numbers are sequential, so this prioritizes apps most likely to have 2017+ office actions
citation_eligible_apps_sorted = sorted(
    citation_eligible_apps,
    key=lambda x: x.get('applicationNumberText', ''),
    reverse=True  # Descending order - highest (most recent) first
)

# Sample for citation analysis (limit to reduce API calls and context usage)
sample_size = min(30, len(citation_eligible_apps_sorted))
sample_apps = citation_eligible_apps_sorted[:sample_size]

print(f"üìä **Sorted by application number (most recent first) for optimal citation coverage**")
print(f"    Sample range: {{sample_apps[0].get('applicationNumberText') if sample_apps else 'N/A'}} to {{sample_apps[-1].get('applicationNumberText') if len(sample_apps) > 1 else 'N/A'}}")
print()

print(f"üìä **Analyzing {{sample_size}} applications for citation patterns...**")
print()
print("‚ö†Ô∏è **IMPORTANT:** Must search ALL {{sample_size}} applications individually - do not skip or aggregate!")
print()

# Aggregate citation data
all_citations = []
examiner_cited_count = 0
applicant_cited_count = 0
apps_with_citations = 0
citation_errors = 0
app_citation_details = []  # Track per-app citation counts for temporal analysis

# CRITICAL: Must iterate through ALL sample_apps - do not stop early even if some have no citations
for i, app in enumerate(sample_apps, 1):
    app_number = app.get('applicationNumberText')

    # Progress indicator - log EVERY application searched
    print(f"  [{{i}}/{{sample_size}}] Searching citations for {{app_number}}...")

    try:
        # Get citations (use application_number only - date filtering automatic)
        citations = search_citations_minimal(
            application_number=app_number,
            rows=100  # Increased to capture all citations
        )

        citation_count = citations.get('response', {{}}).get('numFound', 0)

        if citation_count > 0:
            apps_with_citations += 1
            citation_records = citations.get('response', {{}}).get('docs', [])
            all_citations.extend(citation_records)

            # Extract office action dates for temporal analysis
            oa_dates = [cite.get('officeActionDate', '') for cite in citation_records if cite.get('officeActionDate')]
            oa_dates_formatted = []
            for date_str in oa_dates:
                if date_str:
                    try:
                        # Format as Mon YYYY (e.g., "Jan 2025")
                        from datetime import datetime
                        dt = datetime.strptime(date_str[:10], '%Y-%m-%d')
                        oa_dates_formatted.append(dt.strftime('%b %Y'))
                    except:
                        oa_dates_formatted.append(date_str[:7])  # Fallback to YYYY-MM

            # Store application citation details
            app_citation_details.append({{
                'app_number': app_number,
                'citation_count': citation_count,
                'oa_dates': list(set(oa_dates_formatted))  # Unique dates
            }})

            # Count examiner vs applicant citations
            for cite in citation_records:
                if cite.get('examinerCitedReferenceIndicator') == 'true':
                    examiner_cited_count += 1
                else:
                    applicant_cited_count += 1

    except Exception as e:
        citation_errors += 1
        # Log first few errors for debugging
        if citation_errors <= 3:
            print(f"  ‚ö†Ô∏è Error searching citations for {{app_number}}: {{str(e)[:100]}}")
        # Gracefully continue processing other applications
        continue

print()
print(f"‚úÖ **Citation Analysis Complete**")
print(f"  - Applications with citations: {{apps_with_citations}}/{{sample_size}} ({{(apps_with_citations/sample_size*100):.1f}}%)")
print(f"  - Total citations collected: {{len(all_citations)}}")
if citation_errors > 0:
    print(f"  - Errors/No data: {{citation_errors}} applications")
print()

# Validate citation data sufficiency
citation_coverage = (apps_with_citations / sample_size) * 100 if sample_size > 0 else 0
if citation_coverage < 30:
    print(f"‚ö†Ô∏è **WARNING: Low citation coverage ({{citation_coverage:.1f}}%)**")
    print()
    print("**Possible causes:**")
    print("  - Applications filed before 2015 (pre-coverage period)")
    print("  - Applications without office actions yet")
    print("  - Office actions all occurred before Oct 2017")
    print()
    print("**Recommendation:** Continue with prosecution pattern analysis (citations may be limited)")
    print()

# Temporal Citation Patterns (detailed application-by-application breakdown)
if len(app_citation_details) > 0:
    print("---")
    print()
    print("### Temporal Citation Patterns")
    print()

    # Extract all office action dates for overall range
    all_oa_dates = []
    for app_detail in app_citation_details:
        for oa_date in app_detail['oa_dates']:
            all_oa_dates.append(oa_date)

    if all_oa_dates:
        # Sort dates to find range
        all_oa_dates_sorted = sorted(set(all_oa_dates))
        print(f"**Office Action Dates Range:** {{all_oa_dates_sorted[0]}} to {{all_oa_dates_sorted[-1]}}")
        print()
        print("**Data Coverage Note:** Citation API captures office actions from Oct 1, 2017+ only")
        print()

    print("**Applications Analyzed:**")
    print()

    # Sort by citation count (descending) for better readability
    app_citation_details_sorted = sorted(app_citation_details, key=lambda x: x['citation_count'], reverse=True)

    # Categorize by citation density
    high_density = []
    moderate_density = []
    low_density = []

    for app_detail in app_citation_details_sorted:
        app_num = app_detail['app_number']
        cite_count = app_detail['citation_count']
        oa_dates = app_detail['oa_dates']

        # Format office action dates
        oa_dates_str = ' & '.join(sorted(oa_dates)) if oa_dates else 'Unknown'

        # Categorize by density
        if cite_count >= 10:
            high_density.append(app_num)
            density_note = " - High citation density"
        elif cite_count >= 4:
            moderate_density.append(app_num)
            density_note = ""
        else:
            low_density.append(app_num)
            density_note = ""

        # Special note for highest
        if cite_count == max([a['citation_count'] for a in app_citation_details]):
            density_note = " - Highest citation density"

        print(f"  - **{{app_num}}**: {{cite_count}} citations ({{oa_dates_str}}){{density_note}}")

    print()
    print("**Citation Density Insights:**")
    print()

    if high_density:
        print(f"  - **High-density applications (10+ citations):** {{len(high_density)}} applications")
        print(f"      ‚Üí Suggests complex claim scope or highly competitive technical areas")
        print()

    if moderate_density:
        print(f"  - **Moderate-density (4-9 citations):** {{len(moderate_density)}} applications")
        print(f"      ‚Üí Indicates standard prosecution complexity")
        print()

    if low_density:
        print(f"  - **Low-density (2-3 citations):** {{len(low_density)}} applications")
        print(f"      ‚Üí Suggests narrower claim scope or clearer patentability")
        print()
```

### 2.2 Enhanced Citation Behavior Analysis with Strategic Interpretation

```python
if len(all_citations) > 0:
    total_cites = examiner_cited_count + applicant_cited_count
    examiner_rate = (examiner_cited_count / total_cites) * 100 if total_cites > 0 else 0

    print("### Citation Behavior Metrics")
    print()
    print(f"**Overall Citation Statistics:**")
    print(f"  - Total Citations: {{total_cites}}")
    print(f"  - Examiner-Cited: {{examiner_cited_count}} ({{examiner_rate:.1f}}%)")
    print(f"  - Applicant-Cited: {{applicant_cited_count}} ({{100-examiner_rate:.1f}}%)")
    print(f"  - Citations per Application: {{total_cites / apps_with_citations:.1f}}")
    print(f"  - Examiner Citations per Application: {{examiner_cited_count / apps_with_citations:.1f}}")
    print()

    # Category analysis (examiner-cited only)
    examiner_citations = [c for c in all_citations if c.get('examinerCitedReferenceIndicator') == 'true']

    categories = Counter([
        c.get('citationCategoryCode', 'Unknown')
        for c in examiner_citations
    ])

    print("**Examiner Citation Category Preferences:**")
    for cat, count in categories.most_common():
        pct = (count / examiner_cited_count) * 100 if examiner_cited_count > 0 else 0

        # Enhanced category interpretation with descriptions
        cat_desc = {{
            'X': 'X (Alone anticipates - single reference rejection)',
            'Y': 'Y (Combination anticipates - obviousness rejection)',
            'A': 'A (General background art)',
            'NPL': 'NPL (Non-patent literature)',
            'O': 'O (Oral disclosure)',
            'P': 'P (Intermediate document)',
            'E': 'E (Earlier foreign application)',
            'D': 'D (Document cited in application)',
            'L': 'L (Earlier-filed prior art)',
            'I': 'I (Related to interfering patent)',
            'T': 'T (Later-filed prior art)'
        }}.get(cat, cat)

        print(f"  - {{cat_desc}}: {{count}} ({{pct:.1f}}%)")
    print()

    # Strategic interpretation based on citation patterns
    print("### Strategic Citation Intelligence")
    print()

    # X vs Y balance (anticipation vs obviousness preference)
    x_citations = categories.get('X', 0)
    y_citations = categories.get('Y', 0)
    xy_total = x_citations + y_citations

    if xy_total > 0:
        x_rate = (x_citations / xy_total) * 100
        print("**Rejection Strategy Pattern:**")
        if x_rate > 60:
            print(f"  - ‚ö†Ô∏è **High X-Citation Rate ({{x_rate:.0f}}%):** Examiner frequently finds single-reference anticipation")
            print(f"      ‚Üí **Implication:** Claim scope likely too broad for this examiner")
            print(f"      ‚Üí **Strategy:** Consider narrower claims with specific implementation details")
        elif x_rate < 30:
            print(f"  - ‚úÖ **Low X-Citation Rate ({{x_rate:.0f}}%):** Examiner relies on combination rejections")
            print(f"      ‚Üí **Implication:** Claims avoid single-reference anticipation")
            print(f"      ‚Üí **Strategy:** Focus arguments on lack of motivation to combine and non-obvious differences")
        else:
            print(f"  - üìä **Balanced X/Y Citations ({{x_rate:.0f}}% X, {{100-x_rate:.0f}}% Y):** Mixed anticipation/obviousness approach")
            print(f"      ‚Üí **Strategy:** Prepare for both anticipation and obviousness rejections")
        print()

    # Examiner citation selectivity (IDS strategy guidance)
    print("**Citation Source Selectivity (IDS Strategy Guidance):**")
    if examiner_rate > 75:
        print(f"  - üéØ **Highly Selective ({{examiner_rate:.0f}}% examiner-cited):** Rarely uses applicant-cited references")
        print(f"      ‚Üí **IDS Strategy:** Focus on technical distinctions over comprehensive IDS")
        print(f"      ‚Üí **Rationale:** Examiner conducts own search, rarely adopts applicant references")
        print(f"      ‚Üí **Action:** File targeted IDS with detailed non-applicability explanations")
    elif examiner_rate > 50:
        print(f"  - üìã **Moderately Selective ({{examiner_rate:.0f}}% examiner-cited):** Uses some applicant-cited references")
        print(f"      ‚Üí **IDS Strategy:** Strategic filing of closest prior art with commentary")
        print(f"      ‚Üí **Action:** Include key references with substantive explanations of differences")
    else:
        print(f"  - üìö **Low Selectivity ({{examiner_rate:.0f}}% examiner-cited):** Frequently uses applicant-cited references")
        print(f"      ‚Üí **IDS Strategy:** Comprehensive disclosure can help frame prosecution narrative")
        print(f"      ‚Üí **Action:** Strategic IDS with detailed explanations to shape examiner's understanding")
    print()

    # Citation density (thoroughness indicator)
    avg_examiner_cites = examiner_cited_count / apps_with_citations
    print("**Citation Density (Prior Art Search Thoroughness):**")
    if avg_examiner_cites > 15:
        print(f"  - üìö **High Citation Density ({{avg_examiner_cites:.1f}} citations/app):** Very thorough prior art searcher")
        print(f"      ‚Üí **Implication:** Expect extensive, detailed office actions")
        print(f"      ‚Üí **Strategy:** Prepare comprehensive responses with detailed technical distinctions")
    elif avg_examiner_cites > 8:
        print(f"  - üìä **Moderate Citation Density ({{avg_examiner_cites:.1f}} citations/app):** Standard search thoroughness")
        print(f"      ‚Üí **Strategy:** Standard response approach with clear technical arguments")
    else:
        print(f"  - üìÑ **Low Citation Density ({{avg_examiner_cites:.1f}} citations/app):** Focused search approach")
        print(f"      ‚Üí **Implication:** Examiner identifies strongest references quickly")
        print(f"      ‚Üí **Strategy:** Focus on distinguishing key references cited")
    print()

else:
    print("‚ö†Ô∏è **No citation data available for this examiner**")
    print()
    print("**Possible reasons:**")
    print("  - All applications filed before 2015 (outside citation coverage window)")
    print("  - No office actions issued yet")
    print("  - All office actions occurred before Oct 2017 (pre-citation-data period)")
    print()
    print("üìã **Continuing with prosecution pattern analysis (citations unavailable)...**")
    print()
```

---

## PHASE 3: ALLOWANCE REASONING ANALYSIS (NOA DEEP DIVE)

### 3.1 Strategic NOA Selection for Pattern Detection

```python
print("---")
print()
print("### Allowance Reasoning Analysis (NOA Deep Dive)")
print()

# Find granted patents for NOA analysis
granted_apps = [
    app for app in applications
    if app.get('patentNumber') or
       app.get('applicationMetaData', {{}}).get('appStatusDescText') == 'Patented Case'
]

print(f"üìä **Granted Patents Found:** {{len(granted_apps)}}")
print()

if len(granted_apps) == 0:
    print("‚ö†Ô∏è **No granted patents found for NOA analysis**")
    print("   This examiner may have primarily pending applications")
    print("   Skipping allowance reasoning section...")
    print()
else:
    # Sort by issue date (most recent first)
    granted_sorted = sorted(
        granted_apps,
        key=lambda x: x.get('patentGrantDate', x.get('applicationMetaData', {{}}).get('patentGrantDate', '')),
        reverse=True
    )

    # Select representative sample: recent patents for current examiner behavior
    noa_sample_size = min(5, len(granted_sorted))
    representative_patents = granted_sorted[:noa_sample_size]

    print(f"üìã **Analyzing {{noa_sample_size}} recent NOAs for allowance patterns...**")
    print()

    noa_insights = []
    total_noa_cost = 0.0

    for idx, patent in enumerate(representative_patents, 1):
        app_number = patent.get('applicationNumberText')
        patent_number = patent.get('patentNumber', 'N/A')
        title = patent.get('applicationMetaData', {{}}).get('inventionTitle', 'Unknown')
        issue_date = patent.get('patentGrantDate', patent.get('applicationMetaData', {{}}).get('patentGrantDate', 'N/A'))

        print(f"**NOA {{idx}}/{{noa_sample_size}}: Patent {{patent_number}}**")
        title_display = title[:80] + ('...' if len(title) > 80 else '')
        print(f"  - Title: {{title_display}}")
        print(f"  - Issue Date: {{issue_date}}")

        # Get NOA document
        try:
            noa_docs = pfw_get_application_documents(
                app_number=app_number,
                document_code='NOA',
                limit=1
            )

            if noa_docs.get('documentBag') and len(noa_docs['documentBag']) > 0:
                noa_doc = noa_docs['documentBag'][0]
                page_count = noa_doc.get('pageCount', 'Unknown')
                doc_id = noa_doc.get('documentIdentifier')

                print(f"  - NOA Pages: {{page_count}}")

                # Extract NOA content (auto-optimize: PyPDF2 first, Mistral fallback)
                noa_content = pfw_get_document_content(
                    app_number=app_number,
                    document_identifier=doc_id,
                    auto_optimize=True  # 70% cost savings vs Mistral-only
                )

                extracted_text = noa_content.get('extracted_content', '')
                extraction_method = noa_content.get('extraction_method', 'Unknown')
                cost = noa_content.get('processing_cost_usd', 0.0)
                total_noa_cost += cost

                print(f"  - Extracted: {{len(extracted_text)}} chars via {{extraction_method}}")
                print(f"  - Cost: ${{cost:.3f}}")

                # Store for cross-NOA pattern analysis
                noa_insights.append({{
                    'patent_number': patent_number,
                    'app_number': app_number,
                    'title': title,
                    'noa_text': extracted_text,
                    'page_count': page_count
                }})

                print(f"  - ‚úÖ NOA extracted successfully")
            else:
                print(f"  - ‚ö†Ô∏è NOA document not found in file wrapper")

        except Exception as e:
            print(f"  - ‚ùå Error extracting NOA: {{str(e)[:100]}}")

        print()

    print(f"**Total NOA extraction cost:** ${{total_noa_cost:.3f}}")
    print()

    # Cross-NOA pattern analysis
    if len(noa_insights) > 0:
        print("### Cross-NOA Pattern Analysis")
        print()

        # Common allowance reasoning keywords
        allowance_keywords = {{
            'specification': 0,
            'detailed description': 0,
            'written description': 0,
            'enablement': 0,
            'specific': 0,
            'particular': 0,
            'disclosed': 0,
            'teaches': 0,
            'suggests': 0,
            'motivation': 0,
            'combination': 0,
            'unexpected': 0
        }}

        # Count keyword occurrences across all NOAs
        for noa in noa_insights:
            text_lower = noa['noa_text'].lower()
            for keyword in allowance_keywords:
                if keyword in text_lower:
                    allowance_keywords[keyword] += 1

        # Identify top patterns
        sorted_keywords = sorted(allowance_keywords.items(), key=lambda x: x[1], reverse=True)

        print("**Common Allowance Reasoning Patterns:**")
        for keyword, count in sorted_keywords[:10]:
            if count > 0:
                pct = (count / len(noa_insights)) * 100
                print(f"  - '{{keyword}}': {{count}}/{{len(noa_insights)}} NOAs ({{pct:.0f}}%)")
        print()

        # Strategic recommendations based on NOA patterns
        print("### Allowance Strategy Insights")
        print()

        spec_reliance = allowance_keywords.get('specification', 0) + allowance_keywords.get('detailed description', 0)
        if spec_reliance > len(noa_insights) * 0.6:
            print("**High Specification Reliance Detected:**")
            print(f"  - üìù **Pattern:** Examiner heavily cites specification for claim support ({{spec_reliance}}/{{len(noa_insights)}} NOAs)")
            print(f"      ‚Üí **Claim Drafting:** Ensure detailed description with explicit support for each claim element")
            print(f"      ‚Üí **Specification Strategy:** Include implementation examples for each limitation")
            print(f"      ‚Üí **Prosecution:** Point to specific specification passages in arguments")
            print()

        if allowance_keywords.get('motivation', 0) > len(noa_insights) * 0.4:
            print("**Motivation-Focused Allowances:**")
            print(f"  - üîó **Pattern:** Examiner considers motivation to combine in obviousness analysis")
            print(f"      ‚Üí **Argument Strategy:** Emphasize lack of motivation or teaching away")
            print(f"      ‚Üí **Evidence:** Provide technical reasons why combination would not work or would not be attempted")
            print()

        if allowance_keywords.get('specific', 0) > len(noa_insights) * 0.5:
            print("**Specificity-Oriented Allowances:**")
            print(f"  - üéØ **Pattern:** Examiner allows claims with specific technical details")
            print(f"      ‚Üí **Claim Strategy:** Narrow claims with concrete limitations more likely to succeed")
            print(f"      ‚Üí **Dependent Claims:** Use dependent claims to capture broader scope")
            print()

        if allowance_keywords.get('unexpected', 0) > 0:
            print("**Unexpected Results Recognition:**")
            print(f"  - ‚ú® **Pattern:** Examiner recognizes unexpected results as patentable")
            print(f"      ‚Üí **Prosecution Strategy:** Provide data showing unexpected advantages")
            print(f"      ‚Üí **Specification:** Include comparative examples demonstrating superiority")
            print()
```

---

## PHASE 4: PROSECUTION EFFICIENCY METRICS

### 4.1 RCE and Amendment Analysis

```python
print("---")
print()
print("### Prosecution Patterns & Efficiency Metrics")
print()

# Re-use sample from citation analysis for consistency
prosecution_sample = sample_apps[:min(30, len(sample_apps))]

print(f"üìä **Analyzing {{len(prosecution_sample)}} applications for prosecution patterns...**")
print()

# Initialize counters
rce_count = 0
amendment_count = 0
ctfr_count = 0  # Non-final rejections
ctnf_count = 0  # Final rejections
total_oa_pages = 0
oa_apps = 0

apps_with_rce = []
apps_with_final = []

# Progress tracking
for i, app in enumerate(prosecution_sample, 1):
    app_number = app.get('applicationNumberText')

    if i % 10 == 0:
        print(f"  Progress: {{i}}/{{len(prosecution_sample)}} applications processed...")

    try:
        # Check for RCE filings (continuation after final rejection)
        rce_docs = pfw_get_application_documents(
            app_number=app_number,
            document_code='RCEX',
            limit=10
        )
        if rce_docs.get('documentBag'):
            rce_num = len(rce_docs['documentBag'])
            rce_count += rce_num
            if rce_num > 0:
                apps_with_rce.append(app_number)

        # Check for amendments/responses (applicant activity)
        amend_docs = pfw_get_application_documents(
            app_number=app_number,
            direction_category='INCOMING',  # Applicant submissions
            limit=20
        )
        if amend_docs.get('documentBag'):
            # Filter for actual responses (A... document codes)
            responses = [
                doc for doc in amend_docs['documentBag']
                if doc.get('mailRoomDate') and
                   doc.get('documentCode', '').startswith('A')
            ]
            amendment_count += len(responses)

        # Check for non-final rejections (examiner activity)
        oa_docs = pfw_get_application_documents(
            app_number=app_number,
            document_code='CTFR',  # Non-final rejection
            limit=10
        )
        if oa_docs.get('documentBag'):
            ctfr_count += len(oa_docs['documentBag'])
            oa_apps += 1
            for doc in oa_docs['documentBag']:
                total_oa_pages += doc.get('pageCount', 0)

        # Check for final rejections
        final_docs = pfw_get_application_documents(
            app_number=app_number,
            document_code='CTNF',  # Final rejection
            limit=5
        )
        if final_docs.get('documentBag'):
            ctnf_num = len(final_docs['documentBag'])
            ctnf_count += ctnf_num
            if ctnf_num > 0:
                apps_with_final.append(app_number)

    except Exception as e:
        # Gracefully continue with other applications
        continue

print()
print("‚úÖ **Prosecution pattern analysis complete**")
print()

# Calculate metrics
avg_rce = rce_count / len(prosecution_sample) if len(prosecution_sample) > 0 else 0
rce_rate = (len(apps_with_rce) / len(prosecution_sample)) * 100 if len(prosecution_sample) > 0 else 0
avg_amendments = amendment_count / len(prosecution_sample) if len(prosecution_sample) > 0 else 0
avg_ctfr = ctfr_count / len(prosecution_sample) if len(prosecution_sample) > 0 else 0
avg_ctnf = ctnf_count / len(prosecution_sample) if len(prosecution_sample) > 0 else 0
final_rate = (len(apps_with_final) / len(prosecution_sample)) * 100 if len(prosecution_sample) > 0 else 0
avg_oa_pages = total_oa_pages / oa_apps if oa_apps > 0 else 0

print("### Prosecution Efficiency Metrics")
print()

print("**RCE Analysis (Continuation After Final):**")
print(f"  - Applications with RCE: {{len(apps_with_rce)}}/{{len(prosecution_sample)}} ({{rce_rate:.1f}}%)")
print(f"  - Average RCE per Application: {{avg_rce:.2f}}")
print()

print("**Office Action Patterns:**")
print(f"  - Average Non-Final Rejections: {{avg_ctfr:.2f}} per app")
print(f"  - Average Final Rejections: {{avg_ctnf:.2f}} per app")
print(f"  - Applications with Finals: {{len(apps_with_final)}}/{{len(prosecution_sample)}} ({{final_rate:.1f}}%)")
print(f"  - Average OA Length: {{avg_oa_pages:.1f}} pages")
print()

print("**Applicant Response Activity:**")
print(f"  - Average Amendments/Responses: {{avg_amendments:.2f}} per app")
print()

# Prosecution difficulty assessment
print("### Prosecution Difficulty Assessment")
print()

if avg_rce > 0.8:
    difficulty = "Very High"
    color = "üî¥"
    strategy = "Expect extended prosecution - consider narrow claims from outset"
    budget_multiplier = "3-4x"
elif avg_rce > 0.5:
    difficulty = "High"
    color = "üü†"
    strategy = "Frequent RCE filings - prepare for iterative claim narrowing"
    budget_multiplier = "2-3x"
elif avg_rce > 0.2:
    difficulty = "Moderate"
    color = "üü°"
    strategy = "Some RCE activity - balanced approach with claim flexibility"
    budget_multiplier = "1.5-2x"
else:
    difficulty = "Low"
    color = "üü¢"
    strategy = "Most applications allow without RCE - examiner reasonable"
    budget_multiplier = "1-1.5x"

print(f"**Difficulty Level:** {{color}} **{{difficulty}}**")
print(f"**Expected Budget:** {{budget_multiplier}} standard prosecution costs")
print(f"**Strategy:** {{strategy}}")
print()

if final_rate > 50:
    print(f"‚ö†Ô∏è **High Final Rejection Rate ({{final_rate:.0f}}%):** Examiner frequently issues finals")
    print(f"   ‚Üí **Implication:** Plan for RCE filings or early claim narrowing to avoid finals")
    print()

if avg_oa_pages > 20:
    print(f"üìö **Lengthy Office Actions ({{avg_oa_pages:.0f}} pages avg):** Detailed examiner analysis")
    print(f"   ‚Üí **Implication:** Expect thorough rejections with extensive prior art discussion")
    print(f"   ‚Üí **Strategy:** Prepare comprehensive responses with detailed technical distinctions")
    print()
elif avg_oa_pages > 10:
    print(f"üìÑ **Standard Office Actions ({{avg_oa_pages:.0f}} pages avg):** Normal detail level")
    print()
```

---

## PHASE 5: PETITION HISTORY & QUALITY ASSESSMENT (FPD INTEGRATION)

### 5.1 Petition Analysis for Examiner Quality Indicators

```python
print("---")
print()
print("### Petition History & Quality Assessment (FPD Integration)")
print()

# Search for petitions filed against this examiner's applications
petition_apps = []
petition_count = 0

# Sample 20 applications for petition check (reduce API calls)
petition_sample = prosecution_sample[:20]

print(f"üîç **Checking petition history for {{len(petition_sample)}} applications...**")
print()

for app in petition_sample:
    app_number = app.get('applicationNumberText')

    try:
        petitions = fpd_search_petitions_by_application(
            application_number=app_number,
            include_documents=False  # Metadata only
        )

        if petitions.get('count', 0) > 0:
            petition_count += len(petitions.get('petitions', []))
            petition_apps.append({{
                'app_number': app_number,
                'petitions': petitions['petitions']
            }})
    except Exception as e:
        # FPD data may not be available for all applications
        continue

petition_rate = (len(petition_apps) / len(petition_sample)) * 100 if len(petition_sample) > 0 else 0

print(f"**Petition Activity:**")
print(f"  - Applications with Petitions: {{len(petition_apps)}}/{{len(petition_sample)}} ({{petition_rate:.1f}}%)")
print(f"  - Total Petitions Filed: {{petition_count}}")
print()

if len(petition_apps) > 0:
    # Analyze petition types and outcomes
    petition_types = Counter()
    petition_decisions = Counter()

    for app_data in petition_apps:
        for petition in app_data['petitions']:
            pet_type = petition.get('petitionTypeCode', 'Unknown')
            decision = petition.get('decisionType', 'Unknown')
            petition_types[pet_type] += 1
            petition_decisions[decision] += 1

    print("**Petition Type Distribution:**")
    for pet_type, count in petition_types.most_common():
        pct = (count / petition_count) * 100

        # Interpret petition type
        type_desc = {{
            '182': 'Restriction Requirement Petition',
            '131': 'Terminal Disclaimer Petition',
            '133': 'Suspended Application Petition',
            '137': 'Revival (Unintentional Abandonment)',
            '183': 'Unity of Invention Petition',
            '181': 'Supervisory Review Petition'
        }}.get(pet_type, f'Type {{pet_type}}')

        print(f"  - {{type_desc}}: {{count}} ({{pct:.1f}}%)")
    print()

    print("**Petition Decision Outcomes:**")
    for decision, count in petition_decisions.most_common():
        pct = (count / petition_count) * 100
        print(f"  - {{decision}}: {{count}} ({{pct:.1f}}%)")
    print()

    # Quality indicators based on petition patterns
    print("### Quality Indicators from Petition History")
    print()

    denied_rate = (petition_decisions.get('DENIED', 0) / petition_count) * 100 if petition_count > 0 else 0
    granted_rate = (petition_decisions.get('GRANTED', 0) / petition_count) * 100 if petition_count > 0 else 0

    if petition_rate > 15:
        print(f"‚ö†Ô∏è **High Petition Rate ({{petition_rate:.0f}}%):** Above-average supervisory review requests")
        print(f"   ‚Üí **Implication:** Possible applicant dissatisfaction or procedural challenges")
        print()

    if '181' in petition_types and petition_types['181'] > petition_count * 0.3:
        print(f"üö© **Supervisory Review Petitions ({{petition_types['181']}}):** Applicants seeking examiner oversight")
        print(f"   ‚Üí **Implication:** Possible communication or procedural issues with examiner")
        print(f"   ‚Üí **Strategy:** Maintain clear, documented communication; consider early interviews")
        print()

    if '137' in petition_types and petition_types['137'] > 0:
        print(f"üìÖ **Revival Petitions ({{petition_types['137']}}):** Applications abandoned and revived")
        print(f"   ‚Üí **Implication:** May indicate deadline pressure or procedural challenges")
        print()

    if granted_rate > 50:
        print(f"‚úÖ **High Petition Success Rate ({{granted_rate:.0f}}%):** Examiner actions frequently overturned")
        print(f"   ‚Üí **Implication:** Petitions viable strategy if procedural issues arise")
        print()
    elif denied_rate > 70:
        print(f"‚ùå **Low Petition Success Rate ({{granted_rate:.0f}}%):** Most petitions denied")
        print(f"   ‚Üí **Implication:** Petition strategy less effective; focus on substantive arguments")
        print()

else:
    print("‚úÖ **No Petition History Found:** Clean prosecution record")
    print("   ‚Üí **Positive Indicator:** No evidence of procedural issues or applicant dissatisfaction")
    print()
```

---

## PHASE 6: PTAB CHALLENGE CORRELATION (POST-GRANT RISK)

### 6.1 PTAB Proceedings Analysis for Patent Quality Assessment

**PTAB MCP Tool Changes (as of 2026-01-17):**
- OLD: ptab_search_proceedings_minimal/balanced
- NEW: search_trials_minimal/balanced (for IPR/PGR/CBM)
- NEW: search_appeals_minimal/balanced (for Ex Parte Appeals)
- NEW: search_interferences_minimal/balanced (for Derivations)

**Token Optimization:** Use fields parameter for ultra-minimal queries (99% reduction)
For cross-MCP correlation, request only needed fields:
  fields=['trialNumber', 'trialMetaData.trialStatusCategory', 'petitionerData.petitionerName']
This reduces context from ~40KB (preset minimal) to ~5KB (ultra-minimal)

```python
print("---")
print()
print("### PTAB Challenge Correlation (Post-Grant Risk Assessment)")
print()

# Search for PTAB challenges against granted patents from this examiner
ptab_challenges = []

# Get granted patent numbers from applications
granted_patent_numbers = [
    app.get('patentNumber')
    for app in applications
    if app.get('patentNumber')
]

# Sort patent numbers (descending) - higher numbers = more recent = more likely to have PTAB proceedings
granted_patent_numbers_sorted = sorted(granted_patent_numbers, reverse=True)

print(f"üîç **Checking PTAB challenges for {{len(granted_patent_numbers_sorted)}} granted patents...**")
print()

# Sample to reduce API calls (PTAB data can be extensive)
# Take most recent patents (highest numbers first) - PTAB proceedings more common for recent patents
ptab_sample = granted_patent_numbers_sorted[:30]

if ptab_sample:
    print(f"üìä **PTAB sample range (most recent first):** {{ptab_sample[0]}} to {{ptab_sample[-1] if len(ptab_sample) > 1 else ptab_sample[0]}}")
    print()

for patent_num in ptab_sample:
    try:
        # Use ultra-minimal mode with fields parameter for 99% reduction
        proceedings = search_trials_minimal(
            patent_number=patent_num,
            fields=['trialNumber', 'trialMetaData.trialStatusCategory', 'petitionerData.petitionerName'],
            limit=10
        )

        if proceedings.get('count', 0) > 0:
            ptab_challenges.append({{
                'patent_number': patent_num,
                'proceedings': proceedings['results']
            }})
    except Exception as e:
        # PTAB data may not be available for all patents
        continue

challenge_rate = (len(ptab_challenges) / len(ptab_sample)) * 100 if len(ptab_sample) > 0 else 0

print(f"**PTAB Challenge Statistics:**")
print(f"  - Patents with Challenges: {{len(ptab_challenges)}}/{{len(ptab_sample)}} ({{challenge_rate:.1f}}%)")
print()

if len(ptab_challenges) > 0:
    # Analyze challenge types and outcomes
    proceeding_types = Counter()
    proceeding_statuses = Counter()

    for challenge_data in ptab_challenges:
        for proc in challenge_data['proceedings']:
            proc_type = proc.get('subproceedingType', 'Unknown')
            status = proc.get('proceedingStatusDescriptionText', 'Unknown')
            proceeding_types[proc_type] += 1
            proceeding_statuses[status] += 1

    total_proceedings = sum(proceeding_types.values())

    print("**Challenge Type Distribution:**")
    for proc_type, count in proceeding_types.most_common():
        pct = (count / total_proceedings) * 100
        print(f"  - {{proc_type}}: {{count}} ({{pct:.1f}}%)")
    print()

    print("**Proceeding Outcomes:**")
    for status, count in proceeding_statuses.most_common():
        pct = (count / total_proceedings) * 100
        print(f"  - {{status}}: {{count}} ({{pct:.1f}}%)")
    print()

    # Post-grant risk assessment
    print("### Post-Grant Risk Assessment")
    print()

    if challenge_rate > 20:
        print(f"üö® **High Challenge Rate ({{challenge_rate:.0f}}%):** Patents frequently challenged at PTAB")
        print(f"   ‚Üí **Possible Causes:**")
        print(f"      - Claim quality concerns (overly broad claims)")
        print(f"      - High-value patent space (competitors motivated to challenge)")
        print(f"   ‚Üí **Recommendation:** Comprehensive prior art search and claim refinement")
        print()
    elif challenge_rate > 10:
        print(f"‚ö†Ô∏è **Moderate Challenge Rate ({{challenge_rate:.0f}}%):** Some PTAB activity")
        print(f"   ‚Üí **Implication:** Standard risk level for valuable patents")
        print()
    else:
        print(f"‚úÖ **Low Challenge Rate ({{challenge_rate:.0f}}%):** Minimal PTAB challenges")
        print(f"   ‚Üí **Possible Indicators:**")
        print(f"      - Robust prosecution quality")
        print(f"      - Lower commercial value patent space")
        print(f"      - Patents too recent for challenges")
        print()

    # Outcome analysis
    terminated = proceeding_statuses.get('Terminated', 0)
    instituted = proceeding_statuses.get('Instituted', 0)

    if terminated > instituted:
        print(f"‚úÖ **More Terminations than Institutions:** Patents surviving challenges")
        print()
    elif instituted > 0:
        print(f"‚ö†Ô∏è **Active Institutions:** Some patents under active PTAB review")
        print(f"   ‚Üí **Implication:** Monitor outcomes for examiner quality trends")
        print()

else:
    print("‚úÖ **No PTAB Challenges Found:** Low post-grant risk")
    print()
    print("**Possible Indicators:**")
    print("  - Strong prosecution quality")
    print("  - Lower commercial value space")
    print("  - Patents too recent for challenges (IPR requires post-grant timing)")
    print()
```

---

## PHASE 7: COMPREHENSIVE INTELLIGENCE REPORT

### 7.1 Executive Summary with Key Metrics

```python
print("=" * 80)
print("ENHANCED EXAMINER BEHAVIOR INTELLIGENCE REPORT")
print("=" * 80)
print()

print(f"**Examiner:** {{primary_examiner}}")
print(f"**Primary Art Unit:** {{primary_art_unit[0]}}")
print(f"**Analysis Period:** 2015-01-01 to present")
print(f"**Applications Analyzed:** {{len(applications)}}")
print(f"**Citation Data Coverage:** {{apps_with_citations}}/{{sample_size}} applications ({{citation_coverage:.0f}}%)")
print()

print("### Key Metrics Summary Table")
print()

# Create comprehensive metrics table
print("| Category | Metric | Value |")
print("|----------|--------|-------|")
print(f"| **Citation Behavior** | Examiner Citation Rate | {{examiner_rate:.0f}}% |")
if apps_with_citations > 0:
    print(f"| | Citations per Application | {{examiner_cited_count / apps_with_citations:.1f}} |")
    if categories:
        print(f"| | Primary Category | {{categories.most_common(1)[0][0]}} |")
else:
    print(f"| | Citations per Application | N/A |")
    print(f"| | Primary Category | N/A |")
print(f"| **Prosecution Patterns** | RCE Rate | {{rce_rate:.0f}}% |")
print(f"| | Final Rejection Rate | {{final_rate:.0f}}% |")
print(f"| | Difficulty Level | {{difficulty}} {{color}} |")
print(f"| | Expected Budget | {{budget_multiplier}} standard |")
print(f"| **Quality Indicators** | Petition Rate | {{petition_rate:.0f}}% |")
print(f"| | PTAB Challenge Rate | {{challenge_rate:.0f}}% |")
print()
```

### 7.2 Strategic Prosecution Recommendations

```python
print("---")
print()
print("### STRATEGIC PROSECUTION RECOMMENDATIONS")
print()

print("#### 1. Prior Art & IDS Strategy")
print()

if len(all_citations) > 0:
    if examiner_rate > 70:
        print("**Recommendation:** Focus on technical distinctions over comprehensive IDS")
        print(f"**Rationale:** Examiner rarely uses applicant-cited references ({{examiner_rate:.0f}}% self-cited)")
        print("**Action Items:**")
        print("  - File targeted IDS with key closest prior art only")
        print("  - Include detailed explanations of non-applicability for each reference")
        print("  - Focus prosecution arguments on technical distinctions from examiner's citations")
        print()
    elif examiner_rate > 50:
        print("**Recommendation:** Strategic IDS filing of closest prior art with commentary")
        print(f"**Rationale:** Examiner uses some applicant-cited references ({{examiner_rate:.0f}}% self-cited)")
        print("**Action Items:**")
        print("  - Include key references that show invention's technical advantages")
        print("  - Provide substantive commentary explaining how invention differs")
        print("  - Shape examiner's understanding through strategic disclosure")
        print()
    else:
        print("**Recommendation:** Comprehensive IDS with detailed framing explanations")
        print(f"**Rationale:** Examiner frequently uses applicant-cited references ({{100-examiner_rate:.0f}}% applicant-cited used)")
        print("**Action Items:**")
        print("  - File comprehensive IDS to shape prosecution narrative")
        print("  - Include detailed technical comparison for key references")
        print("  - Use IDS to establish invention's unique technical contribution")
        print()

    # Category-specific guidance
    if categories:
        top_category = categories.most_common(1)[0][0]
        if top_category == 'X':
            print("**Citation Category Focus:** High X-citation preference (single-reference anticipation)")
            print("**Claim Strategy:**")
            print("  - Ensure claims include non-obvious combinations of features")
            print("  - Avoid claims that read on single prior art reference")
            print("  - Include specific implementation details to differentiate from prior art")
            print()
        elif top_category == 'Y':
            print("**Citation Category Focus:** High Y-citation preference (combination rejections)")
            print("**Claim Strategy:**")
            print("  - Emphasize unexpected results and synergistic advantages")
            print("  - Provide evidence of non-obvious technical effects")
            print("  - Include dependent claims with narrowing features for fall-back positions")
            print()
else:
    print("**Note:** Citation data unavailable - use standard comprehensive IDS approach")
    print()

print("#### 2. Claim Drafting Strategy")
print()

if len(noa_insights) > 0:
    spec_reliance_val = allowance_keywords.get('specification', 0) + allowance_keywords.get('detailed description', 0)
    if spec_reliance_val > len(noa_insights) * 0.6:
        print("**Approach:** Specification-heavy claiming with explicit support")
        print("**Detail Level Required:**")
        print("  - Include implementation specifics and concrete examples in claims")
        print("  - Ensure each limitation has dedicated description section in specification")
        print("  - Use specific technical parameters rather than functional language")
        print()
    else:
        print("**Approach:** Standard claim drafting with moderate specification detail")
        print()

    if allowance_keywords.get('specific', 0) > len(noa_insights) * 0.5:
        print("**Scope Guidance:** Narrower claims with specific technical details more likely to succeed")
        print("**Breadth Strategy:**")
        print("  - Independent claims: Specific implementations with concrete limitations")
        print("  - Dependent claims: Capture broader conceptual variations")
        print("  - Multiple independent claims: Cover alternative embodiments specifically")
        print()

print("#### 3. Prosecution Timeline & Budget Planning")
print()

if avg_rce > 0.5:
    print(f"**Expected Duration:** Extended (RCE likely in {{rce_rate:.0f}}% of cases)")
    print(f"**Budget Planning:** Plan for {{budget_multiplier}} standard prosecution costs")
    print("**Strategic Recommendations:**")
    print("  - Consider early claim narrowing to avoid multiple RCE cycles")
    print("  - Prepare fall-back claim sets in advance")
    print("  - Budget for extended prosecution (2-3+ years from first OA)")
    print()
elif avg_rce > 0.2:
    print(f"**Expected Duration:** Standard to extended (RCE in {{rce_rate:.0f}}% of cases)")
    print(f"**Budget Planning:** Plan for {{budget_multiplier}} standard prosecution costs")
    print("**Strategic Recommendations:**")
    print("  - Have contingency claim sets ready")
    print("  - Standard prosecution timeline (1.5-2 years from first OA)")
    print()
else:
    print(f"**Expected Duration:** Standard (RCE uncommon at {{rce_rate:.0f}}%)")
    print("**Budget Planning:** Standard prosecution budget should suffice")
    print("**Strategic Recommendations:**")
    print("  - Examiner reasonable - standard prosecution approach")
    print("  - Expected timeline: 1-1.5 years from first OA to allowance")
    print()

print("#### 4. Office Action Response Strategy")
print()

if avg_oa_pages > 15:
    print(f"**Office Action Detail:** Lengthy, detailed OAs ({{avg_oa_pages:.0f}} pages avg)")
    print("**Response Approach:**")
    print("  - Allocate extra time for comprehensive rebuttal preparation")
    print("  - Match examiner's thoroughness with equally detailed technical responses")
    print("  - Provide point-by-point responses with supporting technical evidence")
    print()
elif avg_oa_pages > 10:
    print(f"**Office Action Detail:** Standard detail level ({{avg_oa_pages:.0f}} pages avg)")
    print("**Response Approach:** Normal response timeline and standard technical arguments")
    print()

if final_rate > 50:
    print(f"**Final OA Strategy:** High final rate ({{final_rate:.0f}}%) - plan for finals")
    print("**Recommendations:**")
    print("  - Consider early claim narrowing to avoid finals")
    print("  - Prepare RCE fall-back claim sets in advance")
    print("  - Budget for after-final responses or RCE continuations")
    print()

print("#### 5. Risk Mitigation Strategies")
print()

if petition_rate > 15:
    print(f"‚ö†Ô∏è **Procedural Risk:** Above-average petition rate ({{petition_rate:.0f}}%)")
    print("**Mitigation Actions:**")
    print("  - Ensure strict deadline compliance (calendar all dates immediately)")
    print("  - Maintain clear, documented communication with examiner")
    print("  - Consider early examiner interview to establish rapport")
    print()

if challenge_rate > 20:
    print(f"‚ö†Ô∏è **Post-Grant Risk:** High PTAB challenge rate ({{challenge_rate:.0f}}%)")
    print("**Mitigation Actions:**")
    print("  - Conduct comprehensive prior art search before filing")
    print("  - Emphasize claim specificity and clear written description")
    print("  - Document technical advantages and unexpected results")
    print()
elif challenge_rate > 10:
    print(f"‚ö†Ô∏è **Post-Grant Risk:** Moderate PTAB activity ({{challenge_rate:.0f}}%)")
    print("**Mitigation Actions:** Standard prior art diligence recommended")
    print()
else:
    print(f"‚úÖ **Low Post-Grant Risk:** Minimal PTAB challenges ({{challenge_rate:.0f}}%)")
    print()

print("#### 6. Examiner Interview Strategy")
print()

if difficulty == "Very High" or difficulty == "High":
    print("**Recommendation:** Schedule early examiner interview after first office action")
    print("**Interview Focus:**")
    print("  - Understand examiner's claim interpretation and specific technical concerns")
    print("  - Clarify scope of prior art rejections and identify allowable subject matter")
    print("  - Present amendment proposals with technical rationale")
    print("**Preparation:**")
    print("  - Bring claim amendment proposals and prior art distinction charts")
    print("  - Prepare technical presentations if complex technology")
    print()
else:
    print("**Recommendation:** Standard interview approach - after first substantive rejection")
    print("**Interview Focus:**")
    print("  - Clarify claim scope and confirm allowable subject matter")
    print("  - Discuss amendment strategy to expedite allowance")
    print()
```

### 7.3 Data Quality & Limitations Disclosure

```python
print("---")
print()
print("### DATA QUALITY & LIMITATIONS")
print()

print("**Coverage Analysis:**")
print(f"  - Applications Analyzed: {{len(applications)}}")
print(f"  - Citation Data Available: {{apps_with_citations}}/{{sample_size}} ({{citation_coverage:.0f}}%)")
print(f"  - Prosecution Data: {{len(prosecution_sample)}} applications")
print(f"  - NOA Analysis: {{len(noa_insights)}} granted patents")
print(f"  - Petition Data: {{len(petition_sample)}} applications checked")
print(f"  - PTAB Data: {{len(ptab_sample)}} granted patents checked")
print()

print("**Data Limitations:**")
print("  - **Citations MCP:** Office actions from Oct 1, 2017+ only (API limitation)")
print("  - **Pre-2017 Applications:** May lack citation data (office actions before coverage period)")
print("  - **Sample Size:** May not capture full examiner behavior range across all technology areas")
print(f"  - **Analysis Period:** Filing dates 2015-01-01 to present (older apps excluded)")
print()

if citation_coverage < 50:
    print("‚ö†Ô∏è **LOW CITATION COVERAGE WARNING:**")
    print(f"   Citation-based recommendations have limited statistical validity ({{citation_coverage:.0f}}% coverage)")
    print("   **Recommendations:**")
    print("     - Request more recent applications (filed 2018+) for better citation coverage")
    print("     - Supplement with manual review of office actions")
    print("     - Focus more weight on prosecution patterns and NOA analysis")
    print()

print("**Statistical Confidence:**")
if len(applications) >= min_sample_size:
    print(f"  ‚úÖ **Sample size ({{len(applications)}}) meets minimum threshold ({{min_sample_size}})**")
    print("     Results have reasonable statistical confidence")
else:
    print(f"  ‚ö†Ô∏è **Sample size ({{len(applications)}}) below recommended minimum ({{min_sample_size}})**")
    print("     **Recommendation:** Increase sample for more robust conclusions")
    print("     Consider broader search or longer time period")
print()
```

### 7.4 Actionable Next Steps

```python
print("---")
print()
print("### RECOMMENDED NEXT STEPS")
print()

print("**Immediate Actions (Before Filing):**")
print("  1. Review NOA text for specific allowance reasoning patterns (completed above)")
print("  2. Analyze representative office actions for claim interpretation style")
print("  3. Prepare initial claim sets following identified success patterns")
print("  4. Conduct prior art search focusing on examiner's preferred citation types")
print()

print("**Pre-Filing Preparation:**")
print("  5. Draft specification with adequate support for all claim limitations")
print("  6. Include concrete examples and specific implementation details")
print("  7. Prepare dependent claims for narrowing strategy if RCE likely")
print("  8. Develop IDS strategy based on examiner's citation selectivity")
print()

print("**During Prosecution:**")
print("  9. Schedule examiner interview after first office action")
print("  10. Prepare amendment proposals aligned with NOA allowance patterns")
print("  11. Provide detailed technical arguments matching examiner's OA detail level")
print()

print("**Ongoing Monitoring:**")
print("  12. Track examiner's recent allowances for evolving patterns")
print("  13. Monitor PTAB challenges if post-grant risk identified")
print("  14. Review petition outcomes if quality concerns flagged")
print()

print("=" * 80)
print("END OF ENHANCED EXAMINER BEHAVIOR INTELLIGENCE REPORT")
print("=" * 80)
print()

print("**Report Generation Notes:**")
print(f"  - Total Applications Retrieved: {{len(applications)}}")
print(f"  - Citations Analyzed: {{len(all_citations)}}")
print(f"  - NOAs Extracted: {{len(noa_insights)}}")
if len(noa_insights) > 0:
    print(f"  - Total Extraction Cost: ${{total_noa_cost:.3f}}")
print()
print("**Next Analysis:** Consider running this analysis periodically (every 6-12 months)")
print("to track examiner behavior changes over time.")
```

---

## IMPLEMENTATION NOTES

### Token Efficiency

**This enhanced workflow uses ultra-minimal mode throughout:**

- **PFW searches:** 8 custom fields (vs 15+ preset) = 47% reduction
- **Citation searches:** 8 preset minimal fields = 90% reduction vs balanced
- **Document retrieval:** Filtered by document_code = 70-90% reduction
- **NOA extraction:** `auto_optimize=True` = 70% cost savings

**Expected Context Usage:**
- 100 applications √ó 8 fields = ~10KB
- 30 citation searches √ó 50 cites √ó 8 fields = ~480KB
- 5 NOA extracts = ~50KB
- **Total: ~540KB** (fits in most LLM context windows)

### Error Handling Philosophy

Every API call wrapped in try-except with:
1. **Graceful degradation:** Continue analysis even if individual calls fail
2. **User notification:** Clear warnings when data unavailable
3. **Statistical validation:** Flag insufficient data for confident conclusions
4. **Progressive disclosure:** User decision points at critical junctures

### Cross-MCP Integration

**Required MCPs:**
- **PFW (Patent File Wrapper):** Application search, document retrieval, NOA extraction
- **Citations (Enriched Citation):** Citation pattern analysis
- **FPD (Petition Decision):** Quality assessment via petition history
- **PTAB (Patent Trial & Appeal Board):** Post-grant risk profiling

**Graceful degradation if MCP unavailable:**
- Citations unavailable ‚Üí Skip citation analysis, continue with prosecution patterns
- FPD unavailable ‚Üí Skip petition analysis, note in limitations
- PTAB unavailable ‚Üí Skip post-grant risk, note in report

### Cost Optimization

**Estimated costs for typical analysis:**
- PFW API calls: Free (public API)
- Citations API calls: Free (public API)
- FPD API calls: Free (public API)
- PTAB API calls: Free (public API)
- **NOA extraction (5 docs):** $0.01-0.05 (auto-optimize mode with PyPDF2 first, Mistral fallback)

**Total estimated cost:** < $0.10 per examiner analysis

---

**COMPREHENSIVE IMPLEMENTATION COMPLETE**

This enhanced prompt provides complete workflows with:
- ‚úÖ Actual tool call examples with parameters
- ‚úÖ Error handling with try-except blocks
- ‚úÖ Progressive disclosure with user decision points
- ‚úÖ Statistical validation and confidence assessment
- ‚úÖ Cross-MCP integration with eligibility checks
- ‚úÖ Presentation formatting (markdown tables, headers)
- ‚úÖ Strategic recommendations based on data patterns
- ‚úÖ Data quality transparency and limitations disclosure
- ‚úÖ Actionable next steps for prosecution planning

**Philosophy:** Invest context in prompt to prevent 10x mistakes during execution.
"""
